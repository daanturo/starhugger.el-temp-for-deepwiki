[[https://melpa.org/#/starhugger][file:https://melpa.org/packages/starhugger-badge.svg]] [[https://stable.melpa.org/#/starhugger][file:https://stable.melpa.org/packages/starhugger-badge.svg]]


- LLM/AI-powered text & code completion client for Emacs.

Currently supported backends: [[https://platform.openai.com/docs/api-reference/completions][OpenAI-compatible API]], [[https://github.com/ollama/ollama][Ollama]].

- [[https://github.com/daanturo/starhugger.el/assets/32123754/5ff826ad-5644-4b15-a811-ad8f3fc78979][Demo]]:
https://github.com/daanturo/starhugger.el/assets/32123754/5ff826ad-5644-4b15-a811-ad8f3fc78979

- Repository: [[https://gitlab.com/daanturo/starhugger.el]]

(Maybe not up to date) Mirror: [[https://github.com/daanturo/starhugger.el]]

* Breaking change!

- [[https://gitlab.com/daanturo/starhugger.el/-/tags/v0.7.0][Version 0.7.0]]: The old Hugging text inference backend was removed, including manual FIM prompt construction.
Use an OpenAI-compatible API provider (the default for now) instead.

* Installation

Starhugger can be installed from [[https://melpa.org/#/starhugger][MELPA]], using the command ~M-x package-install~.

Of if you want to install from source, add one of the following to your configuration:

#+begin_src elisp
;; package-vc.el (built-in from Emacs 29 and above)
(unless (package-installed-p 'starhugger)
  (package-vc-install '(starhugger :url "https://gitlab.com/daanturo/starhugger.el")))

;; straight.el
(straight-use-package '(starhugger :files (:defaults "*.py")))

;; Doom
(package! starhugger :recipe (:files (:defaults "*.py")))

;; elpaca.el
(elpaca (starhugger :repo "https://gitlab.com/daanturo/starhugger.el" :files (:defaults "*.py")))
#+end_src

Or any package manager of your choice.

* Usage

** Basic setup

By default, starhugger use [[https://github.com/ollama/ollama/blob/main/docs/openai.md][Ollama's OpenAI-compatible API]] as the default backend and FIM (fill in the middle) trained models.  For another OpenAI-Compatible provider, you can set the base URL and the corresponding endpoint.

The "legacy" ~/completions~ endpoint supports the ~"suffix"~ parameter but ~/chat/completions~ doesn't.

#+begin_src elisp
;; Example: a Hugging Face Inference provider, note the "/completions" endpoint
(setq starhugger-openai-compat-base-completions-url "https://router.huggingface.co/nebius/v1/completions")

(setq starhugger-openai-compat-api-key "Your API key or access token.")

;; The current default, use it for FIM models
(setq starhugger-completion-backend-function #'starhugger-openai-compat-api-base-completions)

;; The language model's unique on the selected provider, remember this should be
;; a base (not chat) model that was trained with FIM
(setq starhugger-model-id "Qwen/Qwen2.5-Coder-7B-fast")

;;;; Key bindings

;; Fetch and display
(global-set-key (kbd "M-\\") #'starhugger-trigger-suggestion)

(with-eval-after-load 'starhugger
  ;; `starhugger-inline-menu-item' makes a conditional binding that is only active at the inline suggestion start
  (define-key starhugger-inlining-mode-map (kbd "TAB") (starhugger-inline-menu-item #'starhugger-accept-suggestion))
  ;; Cycle among fetched suggestions
  (define-key starhugger-inlining-mode-map (kbd "M-[") (starhugger-inline-menu-item #'starhugger-show-prev-suggestion))
  (define-key starhugger-inlining-mode-map (kbd "M-]") (starhugger-inline-menu-item #'starhugger-show-next-suggestion))
  ;; Partial acceptance
  (define-key starhugger-inlining-mode-map (kbd "M-f") (starhugger-inline-menu-item #'starhugger-accept-suggestion-by-word)))
#+end_src

Use ~starhugger-dismiss-suggestion~ (bound to =C-g= by default when showing suggestion) to cancel.

There is also a buffer-local minor ~starhugger-auto-mode~, but be careful when using it for non-local providers because of their limit rates and/or charging fees.  Consider raising ~starhugger-auto-idle-time~ up.


*Use instruction-tuned models*

There are only so few FIM-trained models, the majority of released LLMs are instruction-tuned, starhugger also tries to support the latter type of models via prompting and post-processing.  

#+begin_src elisp
(setq starhugger-completion-backend-function #'starhugger-openai-compat-api-chat-completions)

;; Current default: Ollama's OpenAI-Compatible API, set appropriately for the provider
(setq starhugger-openai-compat-chat-completions-url "http://localhost:11434/v1/chat/completions")

;; Any instruction-tuned model
(setq starhugger-model-id "gemma3:12b")
#+end_src

To use different system and/or user prompts, customize ~starhugger-instruct-make-prompt-messages-function~.

This prompt is not perfect, especially on smaller models who usually fail to follow the instruction to produce /only/ the filling code.  The post-processing steps (~starhugger-post-process-default-chain~) use some heuristics to extract "just" code from the response.

Beware that reasoning models produce lots of tokens for thinking and take significantly more inference time, you may want to apply some of those settings, depending on the use case and model:

#+begin_src elisp
(setq starhugger-openai-compat-chat-completions-parameter-alist '((reasoning_effort . "low")))

(add-to-list 'starhugger-instruct-default-system-prompts "/no_think")
#+end_src

** Optional settings

#+begin_src elisp
;; For evil users, dismiss after pressing ESC twice
(defvar my-evil-force-normal-state-hook '())
(defun my-evil-run-force-normal-state-hook-after-a (&rest _)
  (run-hooks 'my-evil-force-normal-state-hook))

(advice-add #'evil-force-normal-state
 :after #'my-evil-run-force-normal-state-hook-after-a)

;; Workaround conflict with `blamer.el'
;; (https://github.com/Artawower/blamer.el): when at the end of line, blamer's
;; overlay's `after-string' property will display before starhugger's
;; `display' property, which will result in starhugger's part of suggestion on
;; current line (1) being pushed out of the display

;; <before point>|                            commit info<right edge of the window><suggestion after point, before newline>
;; <the rest of suggestion>

;; workaround: disable `blamer-mode' while `starhugger-inlining-mode'

(defvar-local my-starhugger-inlining-mode--blamer-mode-state nil)
(defvar-local blamer-mode nil)

(defun my-starhugger-inlining-mode-h ()
  (if starhugger-inlining-mode
      (progn
        (add-hook
         'my-evil-force-normal-state-hook #'starhugger-dismiss-suggestion
         nil t)
        (setq my-starhugger-inlining-mode--blamer-mode-state blamer-mode)
        (when my-starhugger-inlining-mode--blamer-mode-state
          (blamer-mode 0)))
    (progn
      (when (and my-starhugger-inlining-mode--blamer-mode-state
                 (not blamer-mode))
        (blamer-mode 1)))))

(add-hook 'starhugger-inlining-mode-hook #'my-starhugger-inlining-mode-h)
#+end_src


** Notes

*** Known quirks

Emacs overlays are used under the hood to display inline suggestion, there are some shortcomings with this approach:

- Not possible to display PRE|<ov>SUF without using 2 different types of overlay properties when SUF isn't emtpy (in the middle of the buffer) and empty (at buffer end)

- At the end of the buffer (overlaystart = overlay-end), the overlay's ~keymap~ property doesn't work

- Conflict with [[https://github.com/Artawower/blamer.el]], mentioned in "Example configuration"

* TODO

- [x] VSCode-like previewing overlays: take after [[https://github.com/zerolfx/copilot.el]].

- [ ] More robust and reliable method to show a different suggestion.

- [-] /Batch-previewing multiple suggestions, maybe with syntax highlighting/.

- [-] Support for auto-completing when typing: investigate Emacs's built-in ~completion-at-point-functions~'s asynchronous capabilities, or another framework?
  Current implementation: ~starhugger-auto-mode~ using overlays.

- [ ] Find a way to take other files into account [[https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html][Copilot Internals | thakkarparth007.github.io]], or a RAG system.

- [-] With [[https://github.com/milanglacier/minuet-ai.el][prompt engineering]], allow making use of conversational models, not just limited to FIM-supported coding ones.

- [ ] Use different backends and/or presets for different commands, for example: auto mode uses a lighter model than the explicit ~starhugger-trigger-suggestion~.
