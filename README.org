[[https://melpa.org/#/starhugger][file:https://melpa.org/packages/starhugger-badge.svg]] [[https://stable.melpa.org/#/starhugger][file:https://stable.melpa.org/packages/starhugger-badge.svg]]


- LLM/AI-powered text & code completion client.

Currently supported backends: [[https://platform.openai.com/docs/api-reference/completions][OpenAI-compatible API]], [[https://github.com/ollama/ollama][Ollama]].

- [[https://github.com/daanturo/starhugger.el/assets/32123754/5ff826ad-5644-4b15-a811-ad8f3fc78979][Demo]]:
https://github.com/daanturo/starhugger.el/assets/32123754/5ff826ad-5644-4b15-a811-ad8f3fc78979

- Repository: [[https://gitlab.com/daanturo/starhugger.el]]

(Maybe not up to date) Mirror: [[https://github.com/daanturo/starhugger.el]]

* Breaking change!

- [[https://gitlab.com/daanturo/starhugger.el/-/tags/v0.6.0][Version 0.7.0]]: The old Hugging text inference backend was removed, including manual FIM prompt construction.
Use an OpenAI-compatible API provider (the default for now) instead.

* Installation

Starhugger can be installed from [[https://melpa.org/#/starhugger][MELPA - Starhugger]], using the command ~package-install~.

Of if you want to install from source, add one of the following to your configuration:

#+begin_src elisp
;; package-vc.el (built-in from Emacs 29 and above)
(unless (package-installed-p 'starhugger)
  (package-vc-install '(starhugger :url "https://gitlab.com/daanturo/starhugger.el")))

;; straight.el
(straight-use-package '(starhugger :files (:defaults "*.py")))

;; Doom
(package! starhugger :recipe (:files (:defaults "*.py")))

;; elpaca.el
(elpaca (starhugger :repo "https://gitlab.com/daanturo/starhugger.el" :files (:defaults "*.py")))
#+end_src

Or any package manager of your choice.

* Usage

** Example configuration

By default, starhugger use [[https://github.com/ollama/ollama/blob/main/docs/openai.md][Ollama's OpenAI-compatible API]] as the default backend and FIM (fill in the middle) trained models.  For another OpenAI-Compatible provider, you can set the base URL and the corresponding endpoint.

The "legacy" ~/completions~ endpoint supports the ~"suffix"~ parameter but ~/chat/completions~ doesn't.

#+begin_src elisp
;; Example: a Hugging Face Inference provider
(setq starhugger-openai-compat-url "https://router.huggingface.co/nebius/v1")

(setq starhugger-openai-compat-base-completions-endpoint "/completions") ; The default, if the provider differs set it accordingly
(setq starhugger-completion-backend-function #'starhugger-openai-compat-api-base-completions) ; The current default, use it for FIM models

;; The language model's unique on the selected provider, remember this should be
;; a base (not instruction-tuned/chat) model that supports FIM
(setq starhugger-model-id "Qwen/Qwen2.5-Coder-7B-fast")

(global-set-key (kbd "M-\\") #'starhugger-trigger-suggestion)

(with-eval-after-load 'starhugger
  ;; `starhugger-inline-menu-item' makes a conditional binding that is only active at the inline suggestion start
  (define-key starhugger-inlining-mode-map (kbd "TAB") (starhugger-inline-menu-item #'starhugger-accept-suggestion))
  (define-key starhugger-inlining-mode-map (kbd "M-[") (starhugger-inline-menu-item #'starhugger-show-prev-suggestion))
  (define-key starhugger-inlining-mode-map (kbd "M-]") (starhugger-inline-menu-item #'starhugger-show-next-suggestion))
  (define-key starhugger-inlining-mode-map (kbd "M-f") (starhugger-inline-menu-item #'starhugger-accept-suggestion-by-word)))
#+end_src


*Use instruction-tuned models*

Since FIM-trained models are limited in number, and the majority of released models are instruction-tuned, starhugger also supports chat models via prompting and post-processing.  

#+begin_src elisp
(setq starhugger-fill-in-the-middle 'instruct) ; Must be set to prepare a suitable prompt for the chat model
(setq starhugger-completion-backend-function #'starhugger-openai-compat-api-chat-completions)

(setq starhugger-openai-compat-url "http://localhost:11434/v1") ; The current default: Ollama's OpenAI-Compatible API
(setq starhugger-openai-compat-chat-completions-endpoint "/chat/completions") ; Set appropriately for the provider

;; Any chat models
(setq starhugger-model-id "gemma3:12b")
#+end_src

To use different system and/or user prompt, customize ~starhugger-instruct-make-messages-prompt-function~.

This is not perfect, especially on smaller models who usually fail to follow the instruction to produce /only/ the filling code, the post-processing steps (~starhugger-post-process-chain~) use some unproven heuristics to extract "just" code from the response.

*Additional settings*

#+begin_src elisp
;; For evil users, dismiss after pressing ESC twice
(defvar my-evil-force-normal-state-hook '())
(defun my-evil-run-force-normal-state-hook-after-a (&rest _)
  (run-hooks 'my-evil-force-normal-state-hook))

(advice-add #'evil-force-normal-state
 :after #'my-evil-run-force-normal-state-hook-after-a)

;; Workaround conflict with `blamer.el'
;; (https://github.com/Artawower/blamer.el): when at the end of line, blamer's
;; overlay's `after-string' property will display before starhugger's
;; `display' property, which will result in starhugger's part of suggestion on
;; current line (1) being pushed out of the display

;; <before point>|                            commit info<right edge of the window><suggestion after point, before newline>
;; <the rest of suggestion>

;; workaround: disable `blamer-mode' while `starhugger-inlining-mode'

(defvar-local my-starhugger-inlining-mode--blamer-mode-state nil)
(defvar-local blamer-mode nil)

(defun my-starhugger-inlining-mode-h ()
  (if starhugger-inlining-mode
      (progn
        (add-hook 'my-evil-force-normal-state-hook
                  (lambda () (starhugger-dismiss-suggestion t))
                  nil t)
        (setq my-starhugger-inlining-mode--blamer-mode-state blamer-mode)
        (when my-starhugger-inlining-mode--blamer-mode-state
          (blamer-mode 0)))
    (progn
      (when (and my-starhugger-inlining-mode--blamer-mode-state
                 (not blamer-mode))
        (blamer-mode 1)))))

(add-hook 'starhugger-inlining-mode-hook #'my-starhugger-inlining-mode-h)
#+end_src

** Primary commands

- Previewing overlay: ~starhugger-trigger-suggestion~ to display the suggestion.

~M-x~ ~starhugger-show-next-suggestion~ and ~starhugger-show-prev-suggestion~ to cycle suggestions.

~M-x~ ~starhugger-accept-suggestion~ to insert current suggestion.

~starhugger-dismiss-suggestion~ (bound to =C-g= by default when showing) to cancel.

There is also ~starhugger-auto-mode~ (non-global minor mode), but take note of its usage for non-local providers because of their limit rates and/or charging fees.

** Notes

*** Known quirks

Emacs overlays are used under the hood to display inline suggestion, there are some shortcomings with this approach:

- Not possible to display PRE|<ov>SUF without using 2 different types of overlay properties when SUF isn't emtpy (in the middle of the buffer) and empty (at buffer end)

- At the end of the buffer (overlaystart = overlay-end), the overlay's ~keymap~ property doesn't work

- Conflict with [[https://github.com/Artawower/blamer.el]], mentioned in "Example configuration"

* TODO

- [x] VSCode-like previewing overlays: take after [[https://github.com/zerolfx/copilot.el]].

- [ ] More robust and reliable method to show a different suggestion.

- [-] /Batch-previewing multiple suggestions, maybe with syntax highlighting/.

- [-] Support for auto-completing when typing: investigate Emacs's built-in ~completion-at-point-functions~'s asynchronous capabilities, or another framework?
  Current implementation: ~starhugger-auto-mode~ using overlays.

- [ ] Find a way to take other files into account [[https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html][Copilot Internals | thakkarparth007.github.io]], or a RAG system.

- [-] With [[https://github.com/milanglacier/minuet-ai.el][prompt engineering]], allow making use of conversational models, not just limited to FIM-supported coding ones.

- [ ] Use different backends and/or presets for different commands, for example: auto mode uses a lighter model than the explicit ~starhugger-trigger-suggestion~.
