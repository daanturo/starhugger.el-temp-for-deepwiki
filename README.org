[[https://melpa.org/#/starhugger][file:https://melpa.org/packages/starhugger-badge.svg]] [[https://stable.melpa.org/#/starhugger][file:https://stable.melpa.org/packages/starhugger-badge.svg]]


Starhugger is an LLM/AI-powered text & code completion client for Emacs.

[[https://github.com/daanturo/starhugger.el/assets/32123754/5ff826ad-5644-4b15-a811-ad8f3fc78979][Demo]]:
https://github.com/daanturo/starhugger.el/assets/32123754/5ff826ad-5644-4b15-a811-ad8f3fc78979

Repository: [[https://gitlab.com/daanturo/starhugger.el]]

(Maybe not up to date) Mirror: [[https://github.com/daanturo/starhugger.el]]


Starhugger supports [[https://platform.openai.com/docs/api-reference/completions][OpenAI-compatible API]] (preferred) and [[https://github.com/ollama/ollama/blob/main/docs/api.md][Ollama's API]] as backends.  On the model side, it supports FIM-trained (fill in the middle) models, instruction-tuned models support is experimental.


* Breaking change!

- [[./CHANGELOG.org::*Version 0.7.0][Version 0.7.0]]: The old Hugging text inference backend and most global options were removed, including manual FIM prompt construction.  Use an OpenAI-compatible API provider with attached options instead.

* Installation

Starhugger can be installed from [[https://melpa.org/#/starhugger][MELPA]], using the command ~M-x package-install~.

Of if you want to install from source, add one of the following to your configuration:

#+begin_src elisp
;; package-vc.el (built-in from Emacs 29 and above)
(unless (package-installed-p 'starhugger)
  (package-vc-install '(starhugger :url "https://gitlab.com/daanturo/starhugger.el")))

;; straight.el
(straight-use-package '(starhugger :files (:defaults "*.py")))

;; Doom
(package! starhugger :recipe (:files (:defaults "*.py")))

;; elpaca.el
(elpaca (starhugger :repo "https://gitlab.com/daanturo/starhugger.el" :files (:defaults "*.py")))
#+end_src

Or any package manager of your choice.

* Usage

** Basic setup

There are 2 primary entrances:
- ~M-x~ ~starhugger-trigger-suggestion~: on-demand command, configured with ~starhugger-interactive-config-instance~.
- ~starhugger-auto-mode~: automatic code suggestion, configured with ~starhugger-auto-config-instance~.

By default, Starhugger use [[https://github.com/ollama/ollama/blob/main/docs/openai.md][Ollama's OpenAI-compatible API]] as the default backend and FIM[fn:openAI-completions] (fill in the middle) trained models.  For another OpenAI-Compatible provider, you can set the configuration appropriately.

[fn:openAI-completions] The "legacy" ~/completions~ endpoint supports the ~"suffix"~ parameter but ~/chat/completions~ doesn't.

#+begin_src elisp
;; Example for the interactive command: a Hugging Face Inference provider, note the "/completions" endpoint for FIM models
(setq starhugger-interactive-config-instance
      (starhugger-config-openai-compat-base-completions
       :url "https://router.huggingface.co/nebius/v1/completions"
       ;; The language model's unique on the selected provider, this should be a
       ;; base (not chat) model that was trained with FIM
       :model "Qwen/Qwen2.5-Coder-7B-fast"
       :api-key "Your API key or access token."
       :code-length 8192))

;; Example for auto mode: use a lighter local provider
(setq starhugger-auto-config-instance
      (starhugger-config-openai-compat-base-completions
       ;; Ollama's OpenAI-compatible server
       :url "http://localhost:11434/v1/completions"
       :model "qwen2.5-coder:3b-base"
       :code-length 1024))

;;;; Key bindings

;; Fetch and display
(global-set-key (kbd "M-\\") #'starhugger-trigger-suggestion)

(with-eval-after-load 'starhugger
  ;; `starhugger-inline-menu-item' makes a conditional binding that is only active at the inline suggestion start
  (define-key starhugger-inlining-mode-map (kbd "TAB") (starhugger-inline-menu-item #'starhugger-accept-suggestion))
  ;; Cycle among fetched suggestions
  (define-key starhugger-inlining-mode-map (kbd "M-[") (starhugger-inline-menu-item #'starhugger-show-prev-suggestion))
  (define-key starhugger-inlining-mode-map (kbd "M-]") (starhugger-inline-menu-item #'starhugger-show-next-suggestion))
  ;; Partial acceptance
  (define-key starhugger-inlining-mode-map (kbd "M-f") (starhugger-inline-menu-item #'starhugger-accept-suggestion-by-word)))
#+end_src

Use ~starhugger-dismiss-suggestion~ (bound to =C-g= by default when showing suggestion) to cancel.

For ~starhugger-auto-mode~, be careful when using it for non-local providers because of their limit rates and/or charging fees.  Consider raising ~starhugger-auto-idle-time~ up.


*Use instruction-tuned models*

There are only so few FIM-trained models and the majority of released LLMs are instruction-tuned, Starhugger also tries to support the latter type of models via prompting and post-processing.  

#+begin_src elisp
(setq starhugger-interactive-config-instance
      (starhugger-config-openai-compat-chat-completions
       :url "http://localhost:11434/v1/chat/completions"
       ;; Any instruction-tuned model
       :model "gemma3:12b"
       ;; Optional: use a custom function other than this to instruct the chat model
       :prompt-params-fn #'starhugger-make-prompt-parameters-default))
#+end_src

The default prompts and post-processing steps may not be perfect, especially on smaller models who usually fail to follow the instruction to produce /only/ the filling code.

To configure, customize any of:

- ~starhugger-instruct-default-system-prompts~: sequence of system prompts, this is used globally by default.
- The config instance's ~:system-prompts~: similar to the above but specific to the config instance, (like a [[https://www.gnu.org/software/emacs/manual/html_node/emacs/Hooks.html][hook]]) a value of ~t~ will be replaced with ~starhugger-instruct-default-system-prompts~ (spliced).
#+begin_src elisp
(setq starhugger-interactive-config-instance
      (starhugger-config-openai-compat-chat-completions
       :url "http://localhost:11434/v1/chat/completions"
       :model "qwen3"
       ;; Example of system prompt: use `starhugger-instruct-default-system-prompts' and softly switch off thinking mode
       :system-prompts '(t "/no_think")
       :parameters '((reasoning_effort . "low"))))
#+end_src
- The config instance's ~:prompt-params-fn~: function to construct parameters in Lisp data, especially the "messages" parameter for OpenAI-compatible APIs.  This is the most flexible way for custom prompting.  Refer to ~starhugger-make-prompt-parameters-default~'s ~starhugger-config-instruct-type-model~ implementation for the expected arguments and return value.
- The config instance's ~:post-process~: a value of ~t~ will be replaced with ~starhugger-post-process-default-chain~.

** Optional settings

#+begin_src elisp
;; For evil users, dismiss after pressing ESC twice
(defvar my-evil-force-normal-state-hook '())
(defun my-evil-run-force-normal-state-hook-after-a (&rest _)
  (run-hooks 'my-evil-force-normal-state-hook))

(advice-add #'evil-force-normal-state
 :after #'my-evil-run-force-normal-state-hook-after-a)

;; Workaround conflict with `blamer.el'
;; (https://github.com/Artawower/blamer.el): when at the end of line, blamer's
;; overlay's `after-string' property will display before starhugger's
;; `display' property, which will result in starhugger's part of suggestion on
;; current line (1) being pushed out of the display

;; <before point>|                            commit info<right edge of the window><suggestion after point, before newline>
;; <the rest of suggestion>

;; workaround: disable `blamer-mode' while `starhugger-inlining-mode'

(defvar-local my-starhugger-inlining-mode--blamer-mode-state nil)
(defvar-local blamer-mode nil)

(defun my-starhugger-inlining-mode-h ()
  (if starhugger-inlining-mode
      (progn
        (add-hook
         'my-evil-force-normal-state-hook #'starhugger-dismiss-suggestion
         nil t)
        (setq my-starhugger-inlining-mode--blamer-mode-state blamer-mode)
        (when my-starhugger-inlining-mode--blamer-mode-state
          (blamer-mode 0)))
    (progn
      (when (and my-starhugger-inlining-mode--blamer-mode-state
                 (not blamer-mode))
        (blamer-mode 1)))))

(add-hook 'starhugger-inlining-mode-hook #'my-starhugger-inlining-mode-h)
#+end_src


** Notes

*** Known quirks

Emacs overlays are used under the hood to display inline suggestion, there are some shortcomings with this approach:

- Not possible to display PRE|<ov>SUF without using 2 different types of overlay properties when SUF isn't emtpy (in the middle of the buffer) and empty (at buffer end)

- At the end of the buffer (overlaystart = overlay-end), the overlay's ~keymap~ property doesn't work

- Conflict with [[https://github.com/Artawower/blamer.el]], mentioned in "Example configuration"

* TODO

- [x] VSCode-like previewing overlays: take after [[https://github.com/zerolfx/copilot.el]].

- [ ] More robust and reliable method to show a different suggestion.

- [-] /Batch-previewing multiple suggestions, maybe with syntax highlighting/.

- [-] Support for auto-completing when typing: investigate Emacs's built-in ~completion-at-point-functions~'s asynchronous capabilities, or another framework?
  Current implementation: ~starhugger-auto-mode~ using overlays.

- [ ] Find a way to take other files into account [[https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html][Copilot Internals | thakkarparth007.github.io]], a RAG system, get from aider, etc.

- [-] With [[https://github.com/milanglacier/minuet-ai.el][prompt engineering]], allow making use of conversational models, not just limited to FIM-supported coding ones.
  Current implementation: clumsy instructions.
